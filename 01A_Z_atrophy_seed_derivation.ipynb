{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nifti Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Directory**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief markup (in Markdown format) that explains the purpose and usage of the `segments_dict`:\n",
    "\n",
    "---\n",
    "\n",
    "## Neuroimaging File Extraction Dictionary\n",
    "\n",
    "The `segments_dict` is a predefined dictionary structured to facilitate the extraction of specific types of neuroimaging files. Each key in the dictionary represents a distinct neuroimaging segment, and its associated value is another dictionary containing the following fields:\n",
    "\n",
    "- **path**: This should be filled with the absolute path to the base directory containing the neuroimaging files for the corresponding segment. \n",
    "- **glob_name_pattern**: This is the string pattern that will be used to \"glob\" or search for the specific files within the provided path. It helps in identifying and extracting the desired files based on their naming conventions.\n",
    "\n",
    "Here's a breakdown of the segments and their respective fields:\n",
    "\n",
    "### 1. Cortical Thickness (Nifti)\n",
    "- **path**: Absolute path to the base directory containing CT files.\n",
    "- **glob_name_pattern**: File pattern to search for CT files.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Please fill out the `path` and `glob_name_pattern` fields for each segment in the `segments_dict`. This will ensure that the extraction process can locate and identify the appropriate neuroimaging files for further analysis.\n",
    "- < *_name_pattern > variables do not need a leading slash (\"/\"). This is already accounted for. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = r'/Volumes/Expansion/datasets/adni/neuroimaging/all_patients_sbm_v2'\n",
    "ct_glob_name_pattern = r'*/vol/*.nii.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder #<----- CALVIN IMPORT\n",
    "\n",
    "def import_ct_dataframes_from_folders(base_directory, ct_glob_name_pattern):\n",
    "    \"\"\"\n",
    "    Imports dataframes from specified directories and glob name patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - grey_matter_glob_name_pattern (str): Glob pattern for grey matter data.\n",
    "    - white_matter_glob_name_pattern (str): Glob pattern for white matter data.\n",
    "    - csf_glob_name_pattern (str): Glob pattern for cerebrospinal fluid data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes for grey matter, white matter, and cerebrospinal fluid.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    segments_dict = {\n",
    "        'ct': {'path': base_directory, 'glob_name_pattern': ct_glob_name_pattern},\n",
    "    }\n",
    "\n",
    "    dataframes_dict = {}\n",
    "\n",
    "    for k, v in segments_dict.items():\n",
    "        dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern'])\n",
    "        print(f'Imported data {k} data with {dataframes_dict[k].shape[0]} voxels and {dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'These are the filenames per subject {dataframes_dict[k].columns}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = import_ct_dataframes_from_folders(base_directory, ct_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Subject ID From File Names**\n",
    "- Using the example filenames that have been printed above, please define a general string:\n",
    "1) Preceding the subject ID. If nothing preceding subject identifier, enter \"\".\n",
    "- Do NOT include mwp[1/2/3] in this. \n",
    "2) Proceeding the subject ID. If nothing proceeding subject identifier, enter \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_id = ''\n",
    "proceeding_id = '_vol_MNI152'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_specific_mwp_integer_pattern(text):\n",
    "    # Define the pattern to search for: 'mwp' followed by [1], [2], or [3]\n",
    "    pattern = r'mwp[123]'\n",
    "    # Replace the first occurrence of the pattern with an empty string\n",
    "    return re.sub(pattern, '', text, count=1)\n",
    "\n",
    "\n",
    "def extract_and_rename_subject_id(dataframe, split_command_dict):\n",
    "    \"\"\"\n",
    "    Renames the columns of a dataframe based on specified split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The dataframe whose columns need to be renamed.\n",
    "    - split_command_dict (dict): A dictionary where the key is the split string \n",
    "                                 and the value is the order to take after splitting \n",
    "                                 (0 for before the split, 1 for after the split, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with renamed columns.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {'subject_001': [1, 2, 3], 'patient_002': [4, 5, 6], 'control_003': [7, 8, 9]}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> split_commands = {'_': 1}\n",
    "    >>> new_df = extract_and_rename_subject_id(df, split_commands)\n",
    "    >>> print(new_df.columns)\n",
    "    Index(['001', '002', '003'], dtype='object')\n",
    "    \"\"\"\n",
    "\n",
    "    raw_names = dataframe.columns\n",
    "    name_mapping = {}\n",
    "\n",
    "    # For each column name in the dataframe\n",
    "    for name in raw_names:\n",
    "        new_name = name  # Default to the original name in case it doesn't match any split command\n",
    "\n",
    "        # Check each split command to see if it applies to this column name\n",
    "        for k, v in split_command_dict.items():\n",
    "            if k in new_name:\n",
    "                new_name = remove_specific_mwp_integer_pattern(new_name)\n",
    "                if k !='':\n",
    "                    new_name = new_name.split(k)[v]\n",
    "        # Add the original and new name to the mapping\n",
    "        name_mapping[name] = new_name\n",
    "\n",
    "    # Rename columns in the dataframe based on the mapping\n",
    "    return dataframe.rename(columns=name_mapping)\n",
    "\n",
    "def rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id):\n",
    "    \"\"\"\n",
    "    Renames the subjects in the provided dataframes based on the split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing dataframes with subjects to be renamed.\n",
    "    - preceding_id (str): The delimiter for taking the part after the split.\n",
    "    - proceeding_id (str): The delimiter for taking the part before the split.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes with subjects renamed.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_command_dict = {preceding_id: 1, proceeding_id: 0}\n",
    "    \n",
    "    for k, v in dataframes_dict.items():\n",
    "        dataframes_dict[k] = extract_and_rename_subject_id(dataframe=dataframes_dict[k], split_command_dict=split_command_dict)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_dfs = rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Control Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory_control = '/Volumes/Expansion/datasets/adni/neuroimaging/true_control/freesurfer_reconall_sbm'\n",
    "control_cortical_thickness_glob_name_pattern = '*/vol/*_vol_MNI152_s6.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dataframes_dict = import_ct_dataframes_from_folders(base_directory_control,control_cortical_thickness_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scored Atrophy Maps for Each Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "def threshold_probabilities(patient_df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    patient_df = patient_df.where(patient_df > threshold, 0)\n",
    "    return patient_df\n",
    "\n",
    "def calculate_z_scores(control_df: pd.DataFrame, patient_df: pd.DataFrame, matter_type=None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to calculate voxel-wise mean, standard deviation for control group and z-scores for patient group.\n",
    "\n",
    "    Args:\n",
    "    control_df (pd.DataFrame): DataFrame where each column represents a control subject, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "    patient_df (pd.DataFrame): DataFrame where each column represents a patient, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "\n",
    "    Returns:\n",
    "    patient_z_scores (pd.DataFrame): DataFrame of voxel-wise z-scores calculated for each patient using control mean and std.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Mask the dataframes to only consider tissues over acceptable probability thresholds\n",
    "    # # Using p>0.2, as typical masking to MNI152 segments uses P > 0.2 for a given segment\n",
    "    \n",
    "    # # Now you can use the function to apply a threshold to patient_df and control_df\n",
    "    threshold = 0.2\n",
    "    patient_df = threshold_probabilities(patient_df, threshold)\n",
    "    control_df = threshold_probabilities(control_df, threshold)\n",
    "\n",
    "    # Calculate mean and standard deviation for each voxel in control group\n",
    "    control_mean = control_df.mean(axis=1)\n",
    "    control_std = control_df.std(axis=1)\n",
    "\n",
    "    # Initialize DataFrame to store patient z-scores\n",
    "    patient_z_scores = pd.DataFrame()\n",
    "\n",
    "    # Calculate z-scores for each patient using control mean and std\n",
    "    for patient in patient_df.columns:\n",
    "        patient_z_scores[patient] = (patient_df[patient] - control_mean) / control_std\n",
    "    return patient_z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_atrophy_dataframes(dataframes_dict, control_dataframes_dict):\n",
    "    \"\"\"\n",
    "    Processes the provided dataframes to calculate z-scores and determine significant atrophy.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing patient dataframes.\n",
    "    - control_dataframes_dict (dict): Dictionary containing control dataframes.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two dictionaries - atrophy_dataframes_dict and significant_atrophy_dataframes_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    atrophy_dataframes_dict = {}\n",
    "    significant_atrophy_dataframes_dict = {}\n",
    "\n",
    "    for k in dataframes_dict.keys():\n",
    "        atrophy_dataframes_dict[k] = calculate_z_scores(control_df=control_dataframes_dict[k], patient_df=dataframes_dict[k])\n",
    "        if k == 'cerebrospinal_fluid':\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] > 2, 0)\n",
    "        else:\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] < -2, 0)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return atrophy_dataframes_dict, significant_atrophy_dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unthresholded_atrophy_dataframes_dict, significant_atrophy_dataframes_dict = process_atrophy_dataframes(dataframes_dict, control_dataframes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Atrophy Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Raw Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti #<-----CAlVIN IMPORT\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_nifti_to_bids(dataframes_dict, bids_base_dir, analysis='tissue_segment_z_scores', ses=None, dry_run=True):\n",
    "    \"\"\"\n",
    "    Saves NIFTI images to a BIDS directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing dataframes with NIFTI data.\n",
    "    - bids_base_dir (str): The base directory where the BIDS structure starts.\n",
    "    - ses (str, optional): Session identifier. If None, defaults to '01'.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes a predefined BIDS directory structure and saves the NIFTI \n",
    "    images accordingly. The function currently has the view_and_save_nifti call commented out \n",
    "    for safety. Uncomment this call if you wish to actually save the NIFTI images.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = { ... }  # some dictionary with dataframes\n",
    "    >>> save_nifti_to_bids(dfs, '/path/to/base/dir')\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in tqdm(dataframes_dict.keys()):\n",
    "        for col in dataframes_dict[k].columns:\n",
    "            \n",
    "            # Define BIDS Directory Architecture\n",
    "            sub_no = col\n",
    "            if ses is None:\n",
    "                ses_no = '01'\n",
    "            else:\n",
    "                ses_no = ses\n",
    "            \n",
    "            # Define and Initialize the Save Directory\n",
    "            out_dir = os.path.join(bids_base_dir, f'sub-{sub_no}', f'ses-{ses_no}', analysis)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            \n",
    "            # Save Image to BIDS Directory\n",
    "            if dry_run:\n",
    "                print(out_dir+f'/sub-{sub_no}_{k}')\n",
    "            else:\n",
    "                view_and_save_nifti(matrix=dataframes_dict[k][col],\n",
    "                                    out_dir=out_dir,\n",
    "                                    output_name=(f'sub-{sub_no}_{k}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Z-Scored Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unthresholded Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = '/Volumes/Expansion/datasets/adni/neuroimaging/all_patients_sbm_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_to_bids(unthresholded_atrophy_dataframes_dict, bids_base_dir=base_directory, analysis='unthresholded_tissue_segment_z_scores', dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholded Maps - The 'Real' Atrophy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_to_bids(significant_atrophy_dataframes_dict, bids_base_dir=base_directory, analysis='thresholded_tissue_segment_z_scores', dry_run=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Done. Enjoy your atrophy seeds.\n",
    "\n",
    "--Calvin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
