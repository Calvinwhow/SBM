{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Atrophy Within Region of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Neuroimaging File Extraction\n",
    "\n",
    "Here's a breakdown of the inputs:\n",
    "\n",
    "### 0. Base Directorty (CSF)\n",
    "- **base_directory**: Absolute path to the base directory containing CSF files.\n",
    "\n",
    "### 1. Cerebrospinal Fluid (CSF)\n",
    "- **glob_name_pattern**: File pattern to search for CSF files.\n",
    "\n",
    "### 2. Grey Matter\n",
    "- **glob_name_pattern**: File pattern to search for grey matter files.\n",
    "\n",
    "### 3. White Matter\n",
    "- **glob_name_pattern**: File pattern to search for white matter files.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Please fill out the `base_directory` and `glob_name_pattern` fields for each segment. This will ensure that the extraction process can locate and identify the appropriate neuroimaging files for further analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Directory\n",
    "- This is the BIDS-style directory which was created by Notebook 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Base Directory\n",
    "base_directory = r'/Volumes/Expansion/datasets/adni/neuroimaging/all_patients_atrophy_seeds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Glob-style path to the subfolder containing niftis of interest\n",
    "- For example, from the base_directory, */*/tissue_segment_z_scores will look for all subjects, all session folders within subjects, and then check the tissue_segment_z_scores folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_glob_path = r'*/*/unthresholded_tissue_segment_z_scores'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list of tissue types to use from within each terminal folder. \n",
    "- tissue_to_import = ['ct']\n",
    "- **Leave as unchanged if you are unsure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tissue_to_import = ['ct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "def import_dataframes_by_tissue(base_directory, shared_glob_path, tissue_to_import):\n",
    "    \"\"\"\n",
    "    Imports dataframes based on tissue types from specified directories and glob name patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - shared_glob_path (str): The shared directory path for all tissues.\n",
    "    - tissue_to_import (list): List of tissues to be imported.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes for each specified tissue.\n",
    "    \"\"\"\n",
    "\n",
    "    segments_dict = {}\n",
    "    for tissue in tissue_to_import:\n",
    "        glob_path = os.path.join(shared_glob_path, ('*'+tissue+'*'))\n",
    "        segments_dict[tissue] = glob_path\n",
    "\n",
    "    dataframes_dict = {}\n",
    "    nan_handler = {'nan': 0, 'posinf':20, 'neginf':-20}\n",
    "    for k, v in segments_dict.items():\n",
    "        dataframes_dict[k] = import_matrices_from_folder(connectivity_path=base_directory, file_pattern=v, convert_nan_to_num=nan_handler)\n",
    "        print(f'Imported data {k} data with {dataframes_dict[k].shape[0]} voxels and {dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'Example filename per subject: {dataframes_dict[k].columns[0]}')\n",
    "        print('\\n--------------------------------\\n')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_dataframes_by_tissue = import_dataframes_by_tissue(base_directory, shared_glob_path, tissue_to_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Subject ID From File Names**\n",
    "Using the example filenames that have been printed above, please define a general string:\n",
    "1) Preceding the subject ID.\n",
    "2) Proceeding the subject ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import extract_and_rename_subject_id\n",
    "def rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id):\n",
    "    \"\"\"\n",
    "    Renames the subjects in the provided dataframes based on the split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing dataframes with subjects to be renamed.\n",
    "    - preceding_id (str): The delimiter for taking the part after the split.\n",
    "    - proceeding_id (str): The delimiter for taking the part before the split.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes with subjects renamed.\n",
    "    \"\"\"\n",
    "\n",
    "    split_command_dict = {preceding_id: 1, proceeding_id: 0}\n",
    "    \n",
    "    for k, v in dataframes_dict.items():\n",
    "        dataframes_dict[k] = extract_and_rename_subject_id(dataframe=dataframes_dict[k], split_command_dict=split_command_dict)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Should Often Be Left Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_id = 'sub-'\n",
    "proceeding_id = '_ct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_atrophy_df_dict = rename_dataframe_subjects(imported_dataframes_by_tissue, preceding_id, proceeding_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Region of Interest Atrophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Region of Interest Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_import_from = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/VBM/rois'\n",
    "shared_glob_pattern = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "region_of_interest_df = import_matrices_from_folder(connectivity_path=folder_to_import_from, file_pattern=shared_glob_pattern)\n",
    "region_of_interest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Damage Scores Per Region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def calculate_damage_scores(thresholded_atrophy_df_dict, region_of_interest_df, count_voxels=False):\n",
    "    \"\"\"\n",
    "    Calculates damage scores for each region of interest based on thresholded atrophy data.\n",
    "    \n",
    "    Parameters:\n",
    "    - thresholded_atrophy_df_dict (dict): Dictionary containing dataframes with thresholded atrophy data.\n",
    "    - region_of_interest_df (DataFrame): DataFrame containing regions of interest.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing damage scores for each region of interest for each patient.\n",
    "    \"\"\"\n",
    "    \n",
    "    damage_df_dict = {}\n",
    "    \n",
    "    for k, v in thresholded_atrophy_df_dict.items():\n",
    "        \n",
    "        # Initialize the Dataframe\n",
    "        damage_df_dict[k] = pd.DataFrame(index=v.columns, columns=region_of_interest_df.columns)\n",
    "        \n",
    "        # Iterate Over Each Region of Interest\n",
    "        for matrix in damage_df_dict[k].columns:\n",
    "            # Extract Damage Score for Each Patient\n",
    "            for subject in damage_df_dict[k].index:\n",
    "                # Mask the subject dataframe to the matrix at hand\n",
    "                intersection = v[subject].where(region_of_interest_df[matrix] > 0, 0)\n",
    "                \n",
    "                # Use multiplication to zero values outside ROI\n",
    "                weighted_overlap = intersection * region_of_interest_df[matrix]\n",
    "                \n",
    "                # Assess overall damage score & atrophied voxels within ROI\n",
    "                damage_score = weighted_overlap.sum()\n",
    "                num_voxels = np.count_nonzero(weighted_overlap)\n",
    "                \n",
    "                # Assign values to DF \n",
    "                damage_df_dict[k].loc[subject, matrix] = damage_score\n",
    "                damage_df_dict[k].loc[subject, 'num_atrophied_voxels_'+matrix] = num_voxels\n",
    "                \n",
    "        print('Dataframe: ', k)\n",
    "        display(damage_df_dict[k])\n",
    "        print('------------- \\n')\n",
    "    \n",
    "    return damage_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df_dict = calculate_damage_scores(thresholded_atrophy_df_dict, region_of_interest_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Total Atrophy Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nimlab import datasets as nimds\n",
    "def calculate_total_atrophy_voxels(thresholded_atrophy_df_dict, damage_df_dict):\n",
    "    \"\"\"\n",
    "    Calculates the total number of atrophy voxels within a mask and updates the damage_df_dict with this information.\n",
    "    \n",
    "    Parameters:\n",
    "    - thresholded_atrophy_df_dict (dict): Dictionary containing dataframes with thresholded atrophy data.\n",
    "    - damage_df_dict (dict): Dictionary containing dataframes to which the 'Total Atrophy Voxels' column will be added.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The updated damage_df_dict with a new column 'Total Atrophy Voxels'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the mask and brain indices\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    mask_data = mni_mask.get_fdata().flatten()\n",
    "    brain_indices = np.where(mask_data > 0)[0]\n",
    "    \n",
    "    for k, v in thresholded_atrophy_df_dict.items():\n",
    "        # initialize the column\n",
    "        damage_df_dict[k]['Total Atrophy Voxels'] = np.nan\n",
    "        for patient_id in v.columns:\n",
    "            # Filter the dataframe using brain indices\n",
    "            filtered_df = v[patient_id].iloc[brain_indices]\n",
    "            damage_df_dict[k].loc[patient_id, 'Total Atrophy Voxels'] = np.count_nonzero(filtered_df)\n",
    "        \n",
    "        print('Dataframe: ', k)\n",
    "        display(damage_df_dict[k])\n",
    "        print('------------- \\n')\n",
    "    \n",
    "    return damage_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df_dict = calculate_total_atrophy_voxels(thresholded_atrophy_df_dict, damage_df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "def sort_dataframes_by_index(damage_df_dict):\n",
    "    \"\"\"\n",
    "    Attempts to convert the index of each dataframe in damage_df_dict to integers \n",
    "    and then sorts the dataframe by its index in ascending order.\n",
    "    \n",
    "    Parameters:\n",
    "    - damage_df_dict (dict): Dictionary containing dataframes to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The sorted damage_df_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_df_dict = {}\n",
    "    \n",
    "    for k, df in damage_df_dict.items():\n",
    "        try:\n",
    "            # Convert index to integers\n",
    "            sorted_index = natsorted(df.index)\n",
    "            sorted_df = df.loc[sorted_index, :]\n",
    "            sorted_df_dict[k] = sorted_df\n",
    "            \n",
    "            # Display the results\n",
    "            print('Dataframe: ', k)\n",
    "            display(sorted_df)\n",
    "            print('------------- \\n')\n",
    "            \n",
    "        except ValueError:\n",
    "            # If conversion to integer fails, just use the original dataframe\n",
    "            sorted_df_dict[k] = df\n",
    "\n",
    "    return sorted_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_damage_df_dict = sort_dataframes_by_index(damage_df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.nifti_utils./Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/VBM/notebooks/02B_W_atrophy_seed_derivation.ipynb import view_and_save_nifti\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_csv_to_bids(dataframes_dict, bids_base_dir, analysis='atrophy_results', ses=None, dry_run=True):\n",
    "    \"\"\"\n",
    "    Saves csv to a BIDS directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing dataframes with NIFTI data.\n",
    "    - bids_base_dir (str): The base directory where the BIDS structure starts.\n",
    "    - ses (str, optional): Session identifier. If None, defaults to '01'.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes a predefined BIDS directory structure and saves the CSV \n",
    "    accordingly.\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, value in tqdm(dataframes_dict.items()):            \n",
    "        # Define BIDS Directory Architecture\n",
    "        sub_no = 'all'\n",
    "        if ses is None:\n",
    "            ses_no = '01'\n",
    "        else:\n",
    "            ses_no = ses\n",
    "        \n",
    "        # Define and Initialize the Save Directory\n",
    "        out_dir = os.path.join(bids_base_dir, 'neuroimaging_analyses', f'ses-{ses_no}', f'sub-{sub_no}', analysis)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Save Image to BIDS Directory\n",
    "        if dry_run:\n",
    "            print(out_dir+f'/{key}.csv')\n",
    "        else:\n",
    "            value.to_csv(out_dir+f'/{key}.csv')\n",
    "            print('Saved to: ', out_dir+f'/{key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis='unthresholded_ct_atrophy_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_to_bids(sorted_damage_df_dict, bids_base_dir=base_directory, analysis=analysis, ses=None, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done. Enjoy your analyses. \n",
    "\n",
    "--Calvin "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
